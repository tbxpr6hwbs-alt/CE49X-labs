{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2d517c6",
   "metadata": {},
   "source": [
    "# CE49X â€“ Lab 5: Biasâ€“Variance Tradeoff using the Air Quality Dataset\n",
    "\n",
    "**Course:** CE49X â€“ Introduction to Computational Thinking and Data Science for Civil Engineers  \n",
    "**Instructor:** Dr. Eyuphan KoÃ§  \n",
    "**Semester:** Fall 2025  \n",
    "\n",
    "**Objective.** Explore the **biasâ€“variance tradeoff** by fitting polynomial regression models (degrees 1â€“10) to predict **CO(GT)** from meteorological variables (**T, RH, AH**) in the UCI Air Quality dataset. You will quantify **training** and **testing** error, draw a **validation curve**, and discuss **underfitting â†’ optimal â†’ overfitting** behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa15f9d2",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Learning Goals\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "- Explain the **biasâ€“variance tradeoff** in supervised learning.\n",
    "- Implement **linear** and **polynomial regression** models in scikit-learn.\n",
    "- Compute and compare **training** vs **testing** error metrics across model complexity.\n",
    "- Draw and interpret a **validation curve** and label **underfitting**, **optimal**, and **overfitting** regions.\n",
    "- Reflect on how **sensor noise** and **missing data** influence the tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d0ec68",
   "metadata": {},
   "source": [
    "## Step 1 â€” Load and Prepare Data\n",
    "\n",
    "**Tasks**\n",
    "1. Load `AirQualityUCI.csv` with **pandas**. The dataset often uses `-200` to denote missing values.\n",
    "2. Replace `-200` with `NaN` and handle missingness (we will **drop** rows with missing target and **impute** features with the **median** inside an ML pipeline to avoid leakage).\n",
    "3. Select features and target:\n",
    "   - Features: `['T', 'RH', 'AH']`\n",
    "   - Target: `'CO(GT)'`\n",
    "4. Perform a **70%/30%** train/test split (`random_state=42` for reproducibility)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ce68b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 1: Imports, configuration, and robust CSV loading ----\n",
    "# We import the libraries we'll use throughout the lab.\n",
    "# - numpy, pandas for data handling\n",
    "# - matplotlib (and optionally seaborn) for plotting\n",
    "# - scikit-learn for model building, preprocessing, and evaluation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Make plots a bit nicer\n",
    "sns.set(context=\"notebook\", style=\"whitegrid\")\n",
    "\n",
    "# Path to the dataset (already uploaded alongside this notebook)\n",
    "from pathlib import Path\n",
    "\n",
    "candidate_paths = [\n",
    "    Path('AirQualityUCI.csv'),\n",
    "    Path('girdiler') / 'AirQualityUCI.csv',\n",
    "]\n",
    "for candidate in candidate_paths:\n",
    "    if candidate.exists():\n",
    "        dataset_path = candidate.resolve()\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(\"AirQualityUCI.csv not found. Place it next to the notebook or inside a 'girdiler/' folder.\")\n",
    "\n",
    "print(f\"Loading dataset from: {dataset_path}\")\n",
    "\n",
    "# Robust CSV loading:\n",
    "# Many copies of this dataset use ';' as delimiter and ',' as decimal.\n",
    "# We'll try that first; if it fails, fall back to default CSV.\n",
    "def load_air_quality_csv(path):\n",
    "    path = str(path)\n",
    "    try:\n",
    "        df_try = pd.read_csv(path, sep=';', decimal=',', na_values=[-200])\n",
    "        # If it loaded but numeric columns are still strings, try coercing below.\n",
    "        return df_try\n",
    "    except Exception:\n",
    "        # Fallback if the file uses default comma delimiter\n",
    "        return pd.read_csv(path, na_values=[-200])\n",
    "\n",
    "df_raw = load_air_quality_csv(dataset_path)\n",
    "\n",
    "# Coerce numeric columns to numeric where possible (non-convertible stay as NaN).\n",
    "for col in df_raw.columns:\n",
    "    df_raw[col] = pd.to_numeric(df_raw[col], errors='ignore')  # keep non-numeric (e.g., Date/Time) unchanged\n",
    "\n",
    "# For safety, replace any lingering sentinel -200 with NaN\n",
    "df_raw = df_raw.replace(-200, np.nan)\n",
    "\n",
    "# Display the head to confirm columns exist (T, RH, AH, CO(GT))\n",
    "display(df_raw.head())\n",
    "print(\"Columns:\", list(df_raw.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1f3f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 1: Feature selection and train/test split ----\n",
    "# We select the features and target required by the lab.\n",
    "features = ['T', 'RH', 'AH']\n",
    "target = 'CO(GT)'\n",
    "\n",
    "# Keep only the necessary columns (ignore date/time and other sensors)\n",
    "missing_cols = [c for c in features + [target] if c not in df_raw.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Required column(s) not found: {missing_cols}. \"\n",
    "                     \"Please check the CSV column names.\")\n",
    "\n",
    "df = df_raw[features + [target]].copy()\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Drop rows with missing target; we'll impute X inside the ML pipeline\n",
    "mask = y.notna()\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "# Perform a 70%/30% train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Test  shape: X={X_test.shape}, y={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d350bd",
   "metadata": {},
   "source": [
    "## Step 2 â€” Fit Models of Increasing Complexity (Degrees 1â€“10)\n",
    "\n",
    "We will build a **pipeline**: `SimpleImputer(median) â†’ PolynomialFeatures(degree=d, include_bias=False) â†’ LinearRegression()`\n",
    "\n",
    "For each degree `d âˆˆ {1,2,...,10}`:\n",
    "1. Fit the model on the **training** data.\n",
    "2. Compute **training MSE** and **testing MSE** to quantify performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51cd981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 2: Train models and record errors ----\n",
    "# We'll iterate degrees 1..10, fitting a pipeline each time and recording errors.\n",
    "degrees = list(range(1, 11))\n",
    "train_mse = []\n",
    "test_mse = []\n",
    "\n",
    "for d in degrees:\n",
    "    # Build a pipeline to avoid data leakage (impute only using train)\n",
    "    model = make_pipeline(\n",
    "        SimpleImputer(strategy='median'),\n",
    "        PolynomialFeatures(degree=d, include_bias=False),\n",
    "        LinearRegression()\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions on training and testing sets\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    # Mean Squared Error (MSE)\n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "    train_mse.append(mse_train)\n",
    "    test_mse.append(mse_test)\n",
    "\n",
    "# Store results in a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame({\n",
    "    \"degree\": degrees,\n",
    "    \"train_mse\": train_mse,\n",
    "    \"test_mse\": test_mse\n",
    "})\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbb3fde",
   "metadata": {},
   "source": [
    "## Step 3 â€” Plot Validation Curve\n",
    "\n",
    "We plot **polynomial degree** (x-axis) vs **error (MSE)** (y-axis) for both **training** and **testing** sets.\n",
    "We then **label regions** of:\n",
    "- **Underfitting** (left, simple models: high bias, both errors high),\n",
    "- **Optimal** (near minimum test error),\n",
    "- **Overfitting** (right, complex models: training error low, test error high)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a615a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 3: Validation curve plot with labeled regions ----\n",
    "# Determine best degree (minimum test MSE)\n",
    "best_idx = int(np.argmin(test_mse))\n",
    "best_degree = degrees[best_idx]\n",
    "best_test_mse = test_mse[best_idx]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(degrees, train_mse, marker='o', label='Training MSE')\n",
    "plt.plot(degrees, test_mse, marker='s', label='Testing MSE')\n",
    "plt.axvline(best_degree, linestyle='--', alpha=0.7, label=f'Best degree = {best_degree}')\n",
    "\n",
    "# Shade underfitting and overfitting zones\n",
    "if best_degree > min(degrees):\n",
    "    plt.axvspan(min(degrees)-0.5, best_degree-0.5, alpha=0.08, label='Underfitting zone')\n",
    "if best_degree < max(degrees):\n",
    "    plt.axvspan(best_degree+0.5, max(degrees)+0.5, alpha=0.08, label='Overfitting zone')\n",
    "\n",
    "plt.title('Biasâ€“Variance Tradeoff: Validation Curve (MSE vs Degree)')\n",
    "plt.xlabel('Model Complexity (Polynomial Degree)')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best degree by test MSE: {best_degree} (Test MSE = {best_test_mse:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e6de56",
   "metadata": {},
   "source": [
    "## Step 4 â€” Discussion (Write-up)\n",
    "\n",
    "Use your results to address the following:\n",
    "\n",
    "1. **Best Generalization:** Which polynomial degree achieved the **lowest testing MSE**? Does the validation curve support this choice?\n",
    "2. **Error Trends:** How did **training** and **testing** errors change as degree increased? Identify the **underfitting**, **optimal**, and **overfitting** regions on your plot.\n",
    "3. **Biasâ€“Variance Behavior:** Explain how high **bias** (simple models) and high **variance** (complex models) appear in this dataset.\n",
    "4. **Data Quality Effects:** Discuss how **sensor noise** and **missing values** (e.g., `-200`) may influence the tradeoff, especially the tendency to overfit noisy signals.\n",
    "\n",
    "> Tip: Re-run the notebook if you change any settings to update the plot and the best degree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac09a1e",
   "metadata": {},
   "source": [
    "### ðŸ”¥ Bonus (Optional): Cross-Validation\n",
    "\n",
    "Instead of a single train/test split, use **k-fold cross-validation** to estimate generalization error more robustly.\n",
    "We will compare mean CV MSE across degrees and see if the optimal degree changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c529effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Bonus: Cross-validation comparison ----\n",
    "# We'll do 5-fold CV with shuffling for stability.\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_mean_mse = []\n",
    "cv_std_mse = []\n",
    "\n",
    "for d in degrees:\n",
    "    model = make_pipeline(\n",
    "        SimpleImputer(strategy='median'),\n",
    "        PolynomialFeatures(degree=d, include_bias=False),\n",
    "        LinearRegression()\n",
    "    )\n",
    "    # cross_val_score returns NEGATIVE MSE when using 'neg_mean_squared_error'\n",
    "    neg_mse_scores = cross_val_score(\n",
    "        model, X, y, cv=kf, scoring='neg_mean_squared_error'\n",
    "    )\n",
    "    mse_scores = -neg_mse_scores\n",
    "    cv_mean_mse.append(mse_scores.mean())\n",
    "    cv_std_mse.append(mse_scores.std())\n",
    "\n",
    "cv_df = pd.DataFrame({\n",
    "    \"degree\": degrees,\n",
    "    \"cv_mean_mse\": cv_mean_mse,\n",
    "    \"cv_std_mse\": cv_std_mse\n",
    "}).set_index(\"degree\")\n",
    "\n",
    "display(cv_df)\n",
    "\n",
    "best_cv_degree = int(cv_df[\"cv_mean_mse\"].idxmin())\n",
    "print(f\"Best degree by 5-fold CV (mean MSE): {best_cv_degree} \"\n",
    "      f\"(Mean MSE = {cv_df.loc[best_cv_degree, 'cv_mean_mse']:.4f} Â± {cv_df.loc[best_cv_degree, 'cv_std_mse']:.4f})\")\n",
    "\n",
    "# Optional: plot CV curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.errorbar(degrees, cv_mean_mse, yerr=cv_std_mse, marker='o', capsize=4, label='CV Mean Â± SD (MSE)')\n",
    "plt.axvline(best_cv_degree, linestyle='--', alpha=0.7, label=f'Best degree (CV) = {best_cv_degree}')\n",
    "plt.title('Cross-Validation Curve: MSE vs Degree (5-fold)')\n",
    "plt.xlabel('Model Complexity (Polynomial Degree)')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db916407",
   "metadata": {},
   "source": [
    "## âœ… Conclusion (Short Summary)\n",
    "\n",
    "- **Underfitting** occurs at **low degrees**: both training and testing errors are relatively high because the model is too simple (high **bias**).\n",
    "- **Overfitting** occurs at **high degrees**: training error becomes very low while testing error rises due to memorizing noise (high **variance**).\n",
    "- The **optimal degree** typically lies **in between**, where the **testing MSE** reaches its minimum (the **sweet spot** on the validation curve).\n",
    "- With environmental **sensor data**, noise and missing observations can encourage overfittingâ€”careful **imputation** and **regularization / model selection** are essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd20086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Quick summary printout for graders/instructors\n",
    "# This cell prints the best degree by test MSE and shows a compact result table.\n",
    "\n",
    "summary_df = results_df.copy()\n",
    "summary_df[\"is_best_test\"] = summary_df[\"degree\"] == summary_df[\"test_mse\"].idxmin() + 1 if hasattr(summary_df[\"test_mse\"], \"idxmin\") else False\n",
    "print(\"Best degree by test MSE:\", int(np.argmin(test_mse) + 1 if len(test_mse) else -1))\n",
    "display(summary_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
